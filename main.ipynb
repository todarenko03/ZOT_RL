{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flappy_bird_env\n",
    "import gymnasium\n",
    "import gym\n",
    "import torch\n",
    "from gymnasium.utils.play import play\n",
    "import numpy as np\n",
    "import pygame\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        ...,\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202]],\n",
       "\n",
       "       [[ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        ...,\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202]],\n",
       "\n",
       "       [[ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        ...,\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202],\n",
       "        [ 78, 192, 202]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]],\n",
       "\n",
       "       [[222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]],\n",
       "\n",
       "       [[222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        ...,\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149],\n",
       "        [222, 216, 149]]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play(env, keys_to_action={(pygame.K_SPACE,): np.array([1])}, noop=np.array([0]),\n",
    "#      fps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_epsilon_greedy_policy(estimator, epsilon, n_action):\n",
    "    def policy_function(state):\n",
    "            probs = torch.ones(n_action) * epsilon / n_action\n",
    "            q_values = estimator.predict(state)\n",
    "            best_action = torch.argmax(q_values).item()\n",
    "            probs[best_action] += 1.0 - epsilon\n",
    "            action = torch.multinomial(probs, 1).item()\n",
    "            return action \n",
    "    return policy_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_Estimate():\n",
    "    def __init__(self, n_feat, n_state, n_action, lr=0.05):#n_state - размерность состояний теперь\n",
    "        self.w, self.b = self.get_gaussian_wb(n_feat,n_state)\n",
    "        self.n_feat = n_feat\n",
    "        self.models = []\n",
    "        self.optimizers = []\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        for _ in range(n_action):\n",
    "            model = torch.nn.Linear(n_feat,1)\n",
    "            self.models.append(model)\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "            self.optimizers.append(optimizer)\n",
    "\n",
    "    def get_gaussian_wb(self, n_feat, n_state, sigma=.2):\n",
    "        torch.manual_seed(0)\n",
    "        w = torch.randn((n_state,n_feat))/sigma\n",
    "        b = torch.rand((n_feat))* 2.0 * math.pi\n",
    "        return w, b\n",
    "\n",
    "    def get_feature(self, s):\n",
    "        features = (2.0 / self.n_feat) ** 0.5 * torch.cos(torch.matmul(torch.tensor(s).float(), self.w) + self.b)\n",
    "        return features\n",
    "\n",
    "    def update(self, s, a, y):\n",
    "        features = Variable(self.get_feature(s))\n",
    "        y_pred = self.models[a](features)\n",
    "\n",
    "        loss = self.criterion(y_pred, Variable(torch.Tensor([y])))\n",
    "\n",
    "        self.optimizers[a].zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizers[a].step()\n",
    "\n",
    "    def predict(self,s):\n",
    "        features = self.get_feature(s)\n",
    "        with torch.no_grad():\n",
    "            return torch.tensor([model(features) for model in self.models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LR_Estimate(10, 3, 10)\n",
    "policy = gen_epsilon_greedy_policy(estimator, 0.1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function gen_epsilon_greedy_policy.<locals>.policy_function at 0x7f989abdff60>\n"
     ]
    }
   ],
   "source": [
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]]], dtype=uint8), {'background': {'upper_left': (0, 0)}, 'pipes': [{'x': 700, 'height': 125, 'top': -515, 'bottom': 325}], 'base': {'x1': 0, 'x2': 672, 'y': 700}, 'bird': {'x': 222, 'y': 376}, 'last_action': 0, 'score': 0})\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(state)\n\u001b[0;32m----> 6\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     new_state, reward, is_done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m      8\u001b[0m     state \u001b[38;5;241m=\u001b[39m new_state\n",
      "Cell \u001b[0;32mIn [7], line 4\u001b[0m, in \u001b[0;36mgen_epsilon_greedy_policy.<locals>.policy_function\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpolicy_function\u001b[39m(state):\n\u001b[1;32m      3\u001b[0m         probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(n_action) \u001b[38;5;241m*\u001b[39m epsilon \u001b[38;5;241m/\u001b[39m n_action\n\u001b[0;32m----> 4\u001b[0m         q_values \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m         best_action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(q_values)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      6\u001b[0m         probs[best_action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m epsilon\n",
      "Cell \u001b[0;32mIn [13], line 35\u001b[0m, in \u001b[0;36mLR_Estimate.predict\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m,s):\n\u001b[0;32m---> 35\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor([model(features) \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels])\n",
      "Cell \u001b[0;32mIn [13], line 21\u001b[0m, in \u001b[0;36mLR_Estimate.get_feature\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_feature\u001b[39m(\u001b[38;5;28mself\u001b[39m, s):\n\u001b[0;32m---> 21\u001b[0m     features \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_feat) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of dict"
     ]
    }
   ],
   "source": [
    "rewards = np.zeros(30)\n",
    "for traj_num in range(30):\n",
    "    state = env.reset()\n",
    "    for step in range(100):\n",
    "        print(state)\n",
    "        action = policy(state)\n",
    "        new_state, reward, is_done, _ = env.step(action)\n",
    "        state = new_state\n",
    "        rewards[traj_num] += reward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
